---
title: 'Assignment 3'
author: "Bailey Bradford, Frances Murray, Jonathan Zisk"
date: "April 2024"
output:
  html_document:
    theme: cosmo 
    toc: true
    toc_float: true
    toc_depth: 4
    code_folding: hide
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, cache = TRUE, warning = FALSE, results = "hide")
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE)

library(dplyr)
library(tidyverse)
library(sf)
library(RSocrata)
library(viridis)
library(spatstat)
library(raster)
library(spdep)
library(FNN)
library(grid)
library(gridExtra)
library(knitr)
library(kableExtra)
library(tidycensus)
library(classInt) 
library(tmap)
# for KDE and ML risk class intervals
# functions
root.dir = "https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/"

source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

```

## Read in Data from Phoenix


```{r load_data}


street_posts <- st_read("https://mapping-phoenix.opendata.arcgis.com/api/download/v1/items/6f41dcc9c5f547569e4ddd35d4b83a51/geojson?layers=0") %>% 
  dplyr::select(OBJECTID) %>% 
  st_transform('ESRI:103223')

police_grid <- st_read("https://mapping-phoenix.opendata.arcgis.com/api/download/v1/items/5bdb7c621a6449bfb6e190e4c09a6636/geojson?layers=0") %>% 
  st_transform(st_crs(street_posts))

phx <- st_union(police_grid) %>% 
    st_transform(st_crs(street_posts))


library(data.table)

#load crime data
crime_data <- fread("https://www.phoenixopendata.com/dataset/cc08aace-9ca9-467f-b6c1-f0879ab1a358/resource/0ce3411a-2fc6-4302-a33f-167f68608a20/download/crime-data_crime-data_crimestat.csv") 

#get rid of space in column names
names(crime_data) <- gsub(x = names(crime_data), pattern = " ", replacement = "_")

#select for car theft and year 2022
car_theft_22 <- crime_data %>% 
  filter(UCR_CRIME_CATEGORY == "MOTOR VEHICLE THEFT" &
    grepl("2022" , OCCURRED_ON)) 

```

## visualizing point data

Plotting point data and density

> How do we analyze point data?

> Are there other geometries useful to represent point locations?

```{r fig.width=6, fig.height=4, results='markup'}


# Plot 2: Density of street lamps with contours overlaid on Phoenix boundary
ggplot() + 
    geom_sf(data = phx, fill = "lightgrey") +  # Add boundary with grey fill
    stat_density2d(data = data.frame(st_coordinates(street_posts)),  # Compute 2D kernel density estimate
                   aes(X, Y, fill = ..level.., alpha = ..level..),  # Define aesthetics for density contours
                   size = 0.01, bins = 40, geom = 'polygon') +  # Set size and number of bins for contours
    scale_colour_brewer(palette = "Reds") +  # Use Viridis color scale for fill
    scale_alpha(range = c(0.00, 0.35), guide = FALSE) +  # Set transparency range for contours
    labs(title = "Density of Street Lamps in Phoenix") +  # Set plot title
    theme_void() + theme(legend.position = "none")  # Use a blank theme and remove legend


```

```{r results='markup', fig.width=6, fig.height=4}

#summarize car theft by police grid

car_theft_22_grid <- car_theft_22 %>% 
  dplyr::select(c(INC_NUMBER, GRID)) %>% 
  group_by(GRID) %>% 
  summarise(n_car_theft = n()) 

#Join car theft to grid 

police_grid_count <- police_grid %>% 
  st_drop_geometry() %>% 
  left_join(car_theft_22_grid, join_by("GRID_NUMBER" == "GRID")) %>% 
  mutate(n_car_theft = if_else(is.na(n_car_theft), 0, n_car_theft)) %>% 
  left_join(police_grid) %>% 
  st_as_sf()


#map of car theft

tmap_mode("plot")

tm_shape(police_grid_count)+ 
  tm_fill("n_car_theft", palette = "PuBuGn", style = "jenks", n= 8)+ 
  tm_layout(main.title = "Car Theft by Police Grid in Phoenix")


```


### Aggregate street lamps to the police grid

```{r spatialjoin}
## add a value of 1 to each crime, sum them with aggregate

street_posts_count <- street_posts %>% 
  dplyr::select(OBJECTID) %>% 
  mutate(count_posts = 1) %>% 
  aggregate(.,police_grid, sum)  %>% 
  mutate(count_posts = replace_na(count_posts, 0)) 


tm_shape(street_posts_count) + 
  tm_fill("count_posts", 
          palette = "Reds", 
          style = "jenks", 
          n = 8)





```

## Nearest Neighbor Feature


```{r knn}
# Convenience aliases to reduce the length of function names
st_c    <- st_coordinates  # Alias for st_coordinates function
st_coid <- st_centroid     # Alias for st_centroid function

# Create nearest neighbor (NN) relationship from abandoned cars data to fishnet grid cells
street_posts_grid <- police_grid_count %>%  # Start with the summarized variables data
    mutate(street_posts.nn = nn_function(  # Create a new column for nearest neighbor information
        st_c(st_coid(police_grid_count)),  # Calculate centroids of fishnet grid cells
        st_c(street_posts),         # Get coordinates of abandoned cars
        k = 10                      # Number of nearest neighbors to find
    ))

```


```{r}

phx_villages <- st_read("https://mapping-phoenix.opendata.arcgis.com/api/download/v1/items/b0c538c2007d4da98aabbe2a25207035/geojson?layers=0") %>% 
  st_transform(st_crs(street_posts))

police_districts <- st_read("https://mapping-phoenix.opendata.arcgis.com/api/download/v1/items/efd9cb91283e4aec906a79cf022a6988/geojson?layers=0") %>% 
  st_transform(st_crs(street_posts))

final_net <-
  st_centroid(street_posts_grid) %>% 
    st_join(dplyr::select(phx_villages, NAME), by = "uniqueID") %>% 
    st_join(dplyr::select(police_districts, NAME), by = "uniqueID") %>%
      st_drop_geometry() %>% 
      left_join(dplyr::select(police_grid, geometry, GRID_NUMBER)) %>%
      st_sf() %>%
  rename("VILLAGE" = NAME.x, 
         "PRECINCT" = NAME.y) %>% 
  left_join(street_posts_count) %>% 
  na.omit()  

```

```{r vizNN}
## Visualize the NN feature
vars_net.long.nn <- 
  dplyr::select(final_net, ends_with(".nn")) %>%
    gather(Variable, value, -geometry)

ggplot() +
      geom_sf(data = vars_net.long.nn, aes(fill=value), colour=NA) +
      scale_fill_viridis(name="NN Distance") +
      labs(title="Street Lamp NN Distance") +
      theme_void()
```

## Local Moran's I for fishnet grid cells

using {spdep} package to to build neighborhood weights and list to calculate local Moran's I.

Note that the code here is *different* than in the book - it has been updated to keep up with changes in packages.

> What is the difference between local and global Moran's I?

A little in depth version of the chunk below can be found:

Mendez C. (2020). Spatial autocorrelation analysis in R. R Studio/RPubs. Available at <https://rpubs.com/quarcs-lab/spatial-autocorrelation>

```{r}
## generates warnings from PROJ issues
## {spdep} to make polygon to neighborhoods... 
final_net.nb <- poly2nb(as_Spatial(final_net), queen=TRUE)
## ... and neighborhoods to list of weights
final_net.weights <- nb2listw(final_net.nb, style="W", zero.policy=TRUE)

# print(final_net.weights, zero.policy=TRUE)
```

```{r}

local_morans <- localmoran(final_net$n_lamps, final_net.weights, zero.policy=TRUE, 
                           alternative = "two.sided") %>% 
  as.data.frame()




### join local Moran's I results to fishnet
final_net.localMorans <- 
  cbind(local_morans, as.data.frame(final_net)) %>% 
  st_sf() %>%
  dplyr::select(street_lamps_count = n_lamps, 
                Local_Morans_I = Ii, 
                P_Value = `Pr(z != E(Ii))`) %>% 
  mutate(Significant_Hotspots = ifelse(P_Value <= 0.001, 1, 0))  %>% 
  gather(Variable, Value, -geometry)

```

### Plotting local Moran's I results

This is a complex code chunk - it's a loop which builds ggplots of local Moran's for each of your `vars`

> What does a significant hot spot tell us about the distribution of burglaries?

```{r fig.width=10, fig.height=4}
## This is just for plotting
vars <- unique(final_net.localMorans$Variable)
varList <- list()

for(i in vars){
  varList[[i]] <- 
    ggplot() +
      geom_sf(data = filter(final_net.localMorans, Variable == i), 
              aes(fill = Value), colour=NA) +
      scale_fill_viridis(name="") +
      labs(title=i) +
      theme_void() + theme(legend.position="bottom")}

do.call(grid.arrange,c(varList, ncol = 4, top = "Local Morans I statistics, Streetlamps"))
```

## Distance to Hot spot

Using NN distance to a hot spot location

```{r}
# generates warning from NN
final_net <- final_net %>% 
  mutate(lamp.isSig = 
           ifelse(local_morans[,5] <= 0.001, 1, 0)) %>%
  mutate(lamp.isSig.dist = 
           nn_function(st_c(st_coid(final_net)),
                       st_c(st_coid(filter(final_net, 
                                           lamp.isSig == 1))), 
                       k = 1))

## What does k = 1 represent?
```

> What does `k = 1` above mean in terms of measuring nearest neighbors?

### Plot NN distance to hot spot

```{r}
ggplot() +
      geom_sf(data = final_net, aes(fill=lamp.isSig.dist), colour=NA) +
      scale_fill_viridis(name="NN Distance") +
      labs(title="Streetlamp NN Distance") +
      theme_void()
```

## Modeling and CV

Leave One Group Out CV on spatial features

```{r results='hide'}

# View(crossValidate)

## define the variables we want
reg.ss.vars <- c("street_posts.nn", "lamp.isSig.dist")

## RUN REGRESSIONS
reg.ss.spatialCV <- crossValidate(
  dataset = final_net,
  id = "GRID_NUMBER",                           
  dependentVariable = "n_car_theft",
  indVariables = reg.ss.vars) %>%
    dplyr::select(cvID = GRID_NUMBER, n_car_theft, Prediction, geometry)
```

```{r}
# calculate errors by GRID

error_by_reg_and_fold <- 
  reg.ss.spatialCV %>%
    group_by(cvID) %>% 
    summarize(Mean_Error = mean(Prediction - n_car_theft, na.rm = T),
              MAE = mean(abs(Mean_Error), na.rm = T),
              SD_MAE = mean(abs(Mean_Error), na.rm = T)) %>%
  ungroup()

error_by_reg_and_fold %>% 
  arrange(desc(MAE))
error_by_reg_and_fold %>% 
  arrange(MAE)

## plot histogram of OOF (out of fold) errors
error_by_reg_and_fold %>%
  ggplot(aes(MAE)) + 
    geom_histogram(bins = 30, colour="black", fill = "#FDE725FF") +
  scale_x_continuous() + 
    labs(title="Distribution of MAE", subtitle = "LOGO-CV",
         x="Mean Absolute Error", y="Count") 

tm_shape(error_by_reg_and_fold)+ 
  tm_polygons(fill = "Mean_Error")
```

## Density vs predictions

The `spatstat` function gets us kernal density estimates with varying search radii.

Note that the code here is *different* than in the book - it has been updated to keep up with changes in packages.

```{r}
# demo of kernel width
burg_ppp <- as.ppp(st_coordinates(street_posts), W = st_bbox(final_net))
burg_KD.1000 <- density.ppp(burg_ppp, 1000)
burg_KD.1500 <- density.ppp(burg_ppp, 1500)
burg_KD.2000 <- density.ppp(burg_ppp, 2000)
burg_KD.df <- rbind(
  mutate(data.frame(rasterToPoints(mask(raster(burg_KD.1000), as(phx_villages, 'Spatial')))), Legend = "1000 Ft."),
  mutate(data.frame(rasterToPoints(mask(raster(burg_KD.1500), as(phx_villages, 'Spatial')))), Legend = "1500 Ft."),
  mutate(data.frame(rasterToPoints(mask(raster(burg_KD.2000), as(phx_villages, 'Spatial')))), Legend = "2000 Ft.")) 

burg_KD.df$Legend <- factor(burg_KD.df$Legend, levels = c("1000 Ft.", "1500 Ft.", "2000 Ft."))

ggplot(data=burg_KD.df, aes(x=x, y=y)) +
  geom_raster(aes(fill=layer)) + 
  facet_wrap(~Legend) +
  coord_sf(crs=st_crs(final_net)) + 
  scale_fill_viridis(name="Density") +
  labs(title = "Kernel density with 3 different search radii") +
  mapTheme(title_size = 14)
```

```{r}

as.data.frame(burg_KD.1000) %>%
  st_as_sf(coords = c("x", "y"), crs = st_crs(final_net)) %>%
  aggregate(., final_net, mean) %>%
   ggplot() +
     geom_sf(aes(fill=value)) +
     geom_sf(data = sample_n(street_posts, 1500), size = .1, col = "red") +
     scale_fill_viridis(name = "Density") +
     labs(title = "Kernel density of lamp posts") +
     mapTheme(title_size = 14)
```

## Get 2018 crime data

Let's see how our model performed relative to KD on the following year's data.

```{r}
car_theft_23 <- crime_data %>% 
  filter(UCR_CRIME_CATEGORY == "MOTOR VEHICLE THEFT" &
    grepl("2023" , OCCURRED_ON)) 


```

# Stoppped here. Not sure how to get the next chunk to work. Not sure I understand kernel density too. 


```{r}
burg_KDE_sum <- as.data.frame(burg_KD.1000) %>%
  st_as_sf(coords = c("x", "y"), crs = st_crs(final_net)) %>%
  aggregate(., final_net, mean) 
kde_breaks <- classIntervals(burg_KDE_sum$value, 
                             n = 5, "fisher")
burg_KDE_sf <- burg_KDE_sum %>%
  mutate(label = "Kernel Density",
         Risk_Category = classInt::findCols(kde_breaks),
         Risk_Category = case_when(
           Risk_Category == 5 ~ "5th",
           Risk_Category == 4 ~ "4th",
           Risk_Category == 3 ~ "3rd",
           Risk_Category == 2 ~ "2nd",
           Risk_Category == 1 ~ "1st")) %>% 
  cbind(
    aggregate(
      dplyr::select(car_theft_23) %>% mutate(n_theft = 1), ., sum) %>%
    mutate(n_theft = replace_na(n_theft, 0))) %>%
  dplyr::select(label, Risk_Category, n_theft)

```

Note that this is different from the book, where we pull a model out of a list of models we've created. For your homework, you'll be creating multiple models.

```{r}
ml_breaks <- classIntervals(reg.ss.spatialCV$Prediction, 
                             n = 5, "fisher")
burg_risk_sf <-
  reg.ss.spatialCV %>%
  mutate(label = "Risk Predictions",
         Risk_Category =classInt::findCols(ml_breaks),
         Risk_Category = case_when(
           Risk_Category == 5 ~ "5th",
           Risk_Category == 4 ~ "4th",
           Risk_Category == 3 ~ "3rd",
           Risk_Category == 2 ~ "2nd",
           Risk_Category == 1 ~ "1st")) %>%
  cbind(
    aggregate(
      dplyr::select(burglaries18) %>% mutate(burgCount = 1), ., sum) %>%
      mutate(burgCount = replace_na(burgCount, 0))) %>%
  dplyr::select(label,Risk_Category, burgCount)
  
```

We don't do quite as well because we don't have very many features, but still pretty good.

```{r}
rbind(burg_KDE_sf, burg_risk_sf) %>%
  na.omit() %>%
  gather(Variable, Value, -label, -Risk_Category, -geometry) %>%
  ggplot() +
    geom_sf(aes(fill = Risk_Category), colour = NA) +
    geom_sf(data = sample_n(burglaries18, 3000), size = .5, colour = "black") +
    facet_wrap(~label, ) +
    scale_fill_viridis(discrete = TRUE) +
    labs(title="Comparison of Kernel Density and Risk Predictions",
         subtitle="2017 burglar risk predictions; 2018 burglaries") +
    mapTheme(title_size = 14)
```

```{r}
rbind(burg_KDE_sf, burg_risk_sf) %>%
  st_drop_geometry() %>%
  na.omit() %>%
  gather(Variable, Value, -label, -Risk_Category) %>%
  group_by(label, Risk_Category) %>%
  summarize(countBurglaries = sum(Value)) %>%
  ungroup() %>%
  group_by(label) %>%
  mutate(Pcnt_of_test_set_crimes = countBurglaries / sum(countBurglaries)) %>%
    ggplot(aes(Risk_Category,Pcnt_of_test_set_crimes)) +
      geom_bar(aes(fill=label), position="dodge", stat="identity") +
      scale_fill_viridis(discrete = TRUE, name = "Model") +
      labs(title = "Risk prediction vs. Kernel density, 2018 burglaries",
           y = "% of Test Set Burglaries (per model)",
           x = "Risk Category") +
  theme_bw() +
      theme(axis.text.x = element_text(angle = 45, vjust = 0.5))
```

## Appendix? 

```{r map of street lamps in dot form}

# Street lights

tm_shape(phx)+ 
  tm_borders()+ 
  tm_shape(street_posts)+ 
  tm_dots(size = .05)+ 
  tm_layout (main.title = "Distribution of streetlamps in Phoenix")



```


## Junk don't keep 

```{r eval=FALSE}

library(mapview)

mapview(final_net, zcol = "PRECINCT", palette = "Spectral")

street_posts_count <- street_posts %>% 
  st_join(police_grid, join = st_within) %>% 
  group_by(GRID_NUMBER) %>% 
  summarise(count = n()) %>% 
  st_drop_geometry() %>%
  rename("n_lamps" = count)

```

